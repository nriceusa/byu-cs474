{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nO8m9UOGEJP"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/porterjenkins/byu-cs474/blob/master/lab11_rl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl7hEuB3J4k3"
   },
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Build DQN and PPO Deep RL algorithms\n",
    "- Learn the difference between Q Learning and Policy Gradient techniques\n",
    "\n",
    "## Deliverable\n",
    "\n",
    "For this lab you will submit an ipython notebook via learning suite. This lab gives you a lot of code, and you should only need to modify two of the cells of this notebook. Feel free to download and modify this notebook or create your own. The below code is given for your convinience. You can modify any of the given code if you wish.\n",
    "\n",
    "## Tips\n",
    "\n",
    "Deep reinforcement learning is difficult. We provide hyperparameters, visualizations, and code for gathering experience, but require you to code up algorithms for training your networks.\n",
    "\n",
    "- Your networks should be able to demonstrate learning on cartpole within a minute of wall time.\n",
    "\n",
    "- Understand what your the starter code is doing. This will help you with the *TODO* sections. The main code block is similar for the two algorithms with some small yet important differences.\n",
    "\n",
    "- We provide hyperparameters for you to start with. Feel free to experiment with different values, but these worked for us.\n",
    "\n",
    "- **Print dtypes and shapes** throughout your code to make sure your tensors look the way you expect.\n",
    "\n",
    "- The DQN algorithm is significantly more unstable than PPO. Even with a correct implementation it may fail to learn every 1/10 times.\n",
    "\n",
    "- Unfortunately visualizing your agent acting in the environment is non-trivial in Colab. You can visualize your agent by running this code locally and uncommenting the `env.render()` line.\n",
    "\n",
    "## Grading\n",
    "\n",
    "- 40% Part 1: DQN *TODO* methods\n",
    "- 40% Part 2: PPO *TODO* methods\n",
    "- 20% Cartpole learning curves\n",
    "\n",
    "\n",
    "### Cartpole\n",
    "\n",
    "Cartpole is a simple environment to get your agent up and running. It has a continuous state space of 4 dimensions and a discrete action space of 2. The agent is given a reward of 1 for each timestep it remains standing. Your agent should be able to reach close to 200 cumulative reward for an episode after a minute or two of training. The below graphs show example results for dqn (left) and ppo (right).\n",
    "\n",
    "![alt text](https://github.com/wingated/cs474_labs/blob/master/images/dqn.png?raw=true)\n",
    "![alt text](https://github.com/wingated/cs474_labs/blob/master/images/ppo.png?raw=true)\n",
    "\n",
    "### TODO\n",
    "\n",
    "- Train DQN and PPO on cartpole\n",
    "- Display learning curves with average episodic reward per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVWokqnVab6O"
   },
   "source": [
    "# Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhaPOG6xt0yn"
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyykNyRM1Tf3",
    "outputId": "7faf796d-67f2-4bb6-cbea-e451fa85aa13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
      "Requirement already satisfied: moviepy==1.0.3 in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy==1.0.3) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy==1.0.3) (2.36.1)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy==1.0.3) (0.5.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy==1.0.3) (4.66.6)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy==1.0.3) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy==1.0.3) (2.32.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy==1.0.3) (0.1.10)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy==1.0.3) (11.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg>=0.2.0->moviepy==1.0.3) (75.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2024.8.30)\n",
      "Requirement already satisfied: imageio_ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg) (75.1.0)\n",
      "Collecting pyvirtualdisplay\n",
      "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pyvirtualdisplay\n",
      "Successfully installed pyvirtualdisplay-3.0\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "The following additional packages will be installed:\n",
      "  freeglut3 libfontenc1 libglu1-mesa libxfont2 libxkbfile1 x11-xkb-utils xfonts-base\n",
      "  xfonts-encodings xfonts-utils xserver-common\n",
      "Suggested packages:\n",
      "  libgle3 python3-numpy\n",
      "The following NEW packages will be installed:\n",
      "  freeglut3 libfontenc1 libglu1-mesa libxfont2 libxkbfile1 python3-opengl x11-xkb-utils xfonts-base\n",
      "  xfonts-encodings xfonts-utils xserver-common xvfb\n",
      "0 upgraded, 12 newly installed, 0 to remove and 49 not upgraded.\n",
      "Need to get 8,639 kB of archives.\n",
      "After this operation, 20.0 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12 [28.7 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12 [864 kB]\n",
      "Fetched 8,639 kB in 1s (10.1 MB/s)\n",
      "Selecting previously unselected package freeglut3:amd64.\n",
      "(Reading database ... 123632 files and directories currently installed.)\n",
      "Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n",
      "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
      "Selecting previously unselected package libfontenc1:amd64.\n",
      "Preparing to unpack .../01-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
      "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
      "Selecting previously unselected package libxfont2:amd64.\n",
      "Preparing to unpack .../02-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
      "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
      "Selecting previously unselected package libxkbfile1:amd64.\n",
      "Preparing to unpack .../03-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
      "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
      "Selecting previously unselected package libglu1-mesa:amd64.\n",
      "Preparing to unpack .../04-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
      "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
      "Selecting previously unselected package python3-opengl.\n",
      "Preparing to unpack .../05-python3-opengl_3.1.5+dfsg-1_all.deb ...\n",
      "Unpacking python3-opengl (3.1.5+dfsg-1) ...\n",
      "Selecting previously unselected package x11-xkb-utils.\n",
      "Preparing to unpack .../06-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
      "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
      "Selecting previously unselected package xfonts-encodings.\n",
      "Preparing to unpack .../07-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
      "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
      "Selecting previously unselected package xfonts-utils.\n",
      "Preparing to unpack .../08-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
      "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
      "Selecting previously unselected package xfonts-base.\n",
      "Preparing to unpack .../09-xfonts-base_1%3a1.0.5_all.deb ...\n",
      "Unpacking xfonts-base (1:1.0.5) ...\n",
      "Selecting previously unselected package xserver-common.\n",
      "Preparing to unpack .../10-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
      "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
      "Selecting previously unselected package xvfb.\n",
      "Preparing to unpack .../11-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
      "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
      "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
      "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
      "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
      "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
      "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
      "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
      "Setting up x11-xkb-utils (7.7+5build4) ...\n",
      "Setting up python3-opengl (3.1.5+dfsg-1) ...\n",
      "Setting up xfonts-utils (1:7.7+6build2) ...\n",
      "Setting up xfonts-base (1:1.0.5) ...\n",
      "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
      "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! pip3 install gymnasium\n",
    "! pip3 install torch\n",
    "! pip install pygame\n",
    "! pip install moviepy==1.0.3\n",
    "! pip install imageio_ffmpeg\n",
    "! pip install pyvirtualdisplay\n",
    "! apt-get install -y xvfb python3-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Rim8iocC1Vva"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium import logger as gymlogger\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "gymlogger.min_level = 40 # Error only\n",
    "\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1f8lZp19Lwj",
    "outputId": "45ec320e-8d9c-4032-d465-3c916c003511"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7c5ed0bafd00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400,900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_TI_5c0l9Ojv"
   },
   "outputs": [],
   "source": [
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else:\n",
    "    print(\"Could not find video\")\n",
    "\n",
    "def wrap_env(env):\n",
    "  env = RecordVideo(env, './video')\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV282uYJ2aSw"
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi_aDdTg2btp"
   },
   "source": [
    "## Part 1\n",
    "\n",
    "\n",
    "Deep Q-Network (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a Q-learning algorithm that learns values for state-action pairs.\n",
    "\n",
    "Actions are sampled according to an $\\epsilon-greedy$ policy to help with exploration of the state space. Every time an action is sampled, the agent chooses a random action with $\\epsilon$ probability. Otherwise, the agent selects the action with the highest Q-value for a state. $\\epsilon$ decays over time according to $\\epsilon \\gets \\epsilon * epsilon\\_decay$.\n",
    "\n",
    "Tuples of state, action, reward, next_state, and terminal $(s,a,r,s',d)$ are collected during training. Every $learn\\_frequency$ steps $sample\\_size$ tuples are sampled and made into 5 tensors tensors of states, actions, rewarads, next_states, and terminals.\n",
    "\n",
    "The loss for a batch of size N is given below.\n",
    "\n",
    "$Loss=\\frac{1}{N}\\sum \\bigg(Q(s,a) - (r + \\gamma \\underset{a'\\sim A}{max} \\hat{Q}(s',a')(1-d))\\bigg)^2 $\n",
    "\n",
    "Loss is calculated and used to update the Q-Network. The target network $\\hat{Q}$ begins as a copy of the Q network but is not updated by the optimizer. Every $target\\_update$ steps, the target network is updated with the parameters of the Q-Network. This process is a type of bootstrapping.\n",
    "\n",
    "### TODO\n",
    "\n",
    "- Implement get action method with e-greedy policy\n",
    "- Implement sample batch method\n",
    "- Implement DQN learning algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mBUvXkT2dHy"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "did you forget parentheses around the comprehension target? (3713041807.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 42\u001b[0;36m\u001b[0m\n\u001b[0;31m    s, a, n, r, d for s, a, n, r, d in batch\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m did you forget parentheses around the comprehension target?\n"
     ]
    }
   ],
   "source": [
    "def get_action_dqn(network, state, epsilon, epsilon_decay):\n",
    "  \"\"\"Select action according to e-greedy policy and decay epsilon\n",
    "    Args:\n",
    "        network (QNetwork): Q-Network\n",
    "        state (np-array): current state, size (state_size)\n",
    "        epsilon (float): probability of choosing a random action\n",
    "        epsilon_decay (float): amount by which to decay epsilon\n",
    "\n",
    "    Returns:\n",
    "        action (int): chosen action [0, action_size)\n",
    "        epsilon (float): decayed epsilon\n",
    "  \"\"\"\n",
    "  if random.random() < epsilon:\n",
    "    # Explore / random action\n",
    "    action = np.random.choice(state)\n",
    "  else:\n",
    "    # Exploit / greedy action\n",
    "    q_values = network(state)\n",
    "    action = torch.argmax(q_values).item()\n",
    "\n",
    "  epsilon *= epsilon_decay\n",
    "  \n",
    "  return action, epsilon\n",
    "\n",
    "\n",
    "def prepare_batch(memory, batch_size):\n",
    "  \"\"\"Randomly sample batch from memory\n",
    "     Prepare cuda tensors\n",
    "\n",
    "    Args:\n",
    "        memory (list): state, action, next_state, reward, done tuples\n",
    "        batch_size (int): amount of memory to sample into a batch\n",
    "\n",
    "    Returns:\n",
    "        state (tensor): float cuda tensor of size (batch_size x state_size)\n",
    "        action (tensor): long tensor of size (batch_size)\n",
    "        next_state (tensor): float cuda tensor of size (batch_size x state_size)\n",
    "        reward (tensor): float cuda tensor of size (batch_size)\n",
    "        done (tensor): float cuda tensor of size (batch_size)\n",
    "  \"\"\"\n",
    "  batch = random.sample(memory, batch_size)\n",
    "  states, actions, next_states, rewards, dones = zip(*batch)  # Revisit\n",
    "  \n",
    "  states = torch.tensor(states, dtype=torch.float).cuda()\n",
    "  actions = torch.tensor(actions, dtype=torch.long).cuda()\n",
    "  next_states = torch.tensor(next_states, dtype=torch.float).cuda()\n",
    "  rewards = torch.tensor(rewards, dtype=torch.float).cuda()\n",
    "  dones = torch.tensor(dones, dtype=torch.float).cuda()\n",
    "\n",
    "  return states, actions, next_states, rewards, dones\n",
    "\n",
    "\n",
    "def learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update):\n",
    "  \"\"\"Update Q-Network according to DQN Loss function\n",
    "     Update Target Network every target_update global steps\n",
    "\n",
    "    Args:\n",
    "        batch (tuple): tuple of state, action, next_state, reward, and done tensors\n",
    "        optim (Adam): Q-Network optimizer\n",
    "        q_network (QNetwork): Q-Network\n",
    "        target_network (QNetwork): Target Q-Network\n",
    "        gamma (float): discount factor\n",
    "        global_step (int): total steps taken in environment\n",
    "        target_update (int): frequency of target network update\n",
    "  \"\"\"\n",
    "  states, actions, next_states, rewards, dones = batch\n",
    "\n",
    "  q_values = q_network(states)\n",
    "  q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # Revisit\n",
    "\n",
    "  next_q_values = target_network(next_states)\n",
    "  next_q_value = next_q_values.max(dim=1)[0]\n",
    "\n",
    "  target_q_values = rewards + gamma * next_q_value * (1 - dones)\n",
    "  loss = F.mse_loss(q_values, target_q_values.detach())\n",
    "\n",
    "  optim.zero_grad()\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "\n",
    "  if global_step % target_update == 0:\n",
    "    target_network.load_state_dict(q_network.state_dict())  # Revisit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGQgiY0WvImB"
   },
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vMhl-oevIBo"
   },
   "outputs": [],
   "source": [
    "# Q-Value Network\n",
    "class QNetwork(nn.Module):\n",
    "  def __init__(self, state_size, action_size):\n",
    "    super().__init__()\n",
    "    hidden_size = 8\n",
    "\n",
    "    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_size, hidden_size),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_size, hidden_size),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_size, action_size))\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Estimate q-values given state\n",
    "\n",
    "      Args:\n",
    "          state (tensor): current state, size (batch x state_size)\n",
    "\n",
    "      Returns:\n",
    "          q-values (tensor): estimated q-values, size (batch x action_size)\n",
    "    \"\"\"\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCafVI552dgg"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Sy_r9Wr2eg8"
   },
   "outputs": [],
   "source": [
    "def dqn_main():\n",
    "  # Hyper parameters\n",
    "  lr = 1e-3\n",
    "  epochs = 500\n",
    "  start_training = 1000\n",
    "  gamma = 0.99\n",
    "  batch_size = 32\n",
    "  epsilon = 1\n",
    "  epsilon_decay = .9999\n",
    "  target_update = 1000\n",
    "  learn_frequency = 2\n",
    "\n",
    "  # Init environment\n",
    "  state_size = 4\n",
    "  action_size = 2\n",
    "  env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "  # Init networks\n",
    "  q_network = QNetwork(state_size, action_size).cuda()\n",
    "  target_network = QNetwork(state_size, action_size).cuda()\n",
    "  target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "  # Init optimizer\n",
    "  optim = torch.optim.Adam(q_network.parameters(), lr=lr)\n",
    "\n",
    "  # Init replay buffer\n",
    "  memory = []\n",
    "\n",
    "  # Begin main loop\n",
    "  results_dqn = []\n",
    "  global_step = 0\n",
    "  loop = tqdm(total=epochs, position=0, leave=False)\n",
    "  for epoch in range(epochs):\n",
    "    last_epoch = (epoch+1 == epochs)\n",
    "    # Record the last epoch, not the previous epochs\n",
    "    if last_epoch:\n",
    "      env = wrap_env(env)\n",
    "\n",
    "    # Reset environment\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    cum_reward = 0  # Track cumulative reward per episode\n",
    "\n",
    "    # Begin episode\n",
    "    while not done and cum_reward < 200:  # End after 200 steps\n",
    "      # Select e-greedy action\n",
    "      action, epsilon = get_action_dqn(q_network, state, epsilon, epsilon_decay)\n",
    "\n",
    "      # Take step\n",
    "      next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "      done = terminated or truncated\n",
    "      # env.render()\n",
    "\n",
    "      # Store step in replay buffer\n",
    "      memory.append((state, action, next_state, reward, done))\n",
    "\n",
    "      cum_reward += reward\n",
    "      global_step += 1  # Increment total steps\n",
    "      state = next_state  # Set current state\n",
    "\n",
    "      # If time to train\n",
    "      if global_step > start_training and global_step % learn_frequency == 0:\n",
    "\n",
    "        # Sample batch\n",
    "        batch = prepare_batch(memory, batch_size)\n",
    "\n",
    "        # Train\n",
    "        learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update)\n",
    "    env.close()\n",
    "    # Print results at end of episode\n",
    "    results_dqn.append(cum_reward)\n",
    "    loop.update(1)\n",
    "    loop.set_description('Episodes: {} Reward: {}'.format(epoch, cum_reward))\n",
    "\n",
    "  return results_dqn\n",
    "\n",
    "results_dqn = dqn_main()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWYwytCDC3aw"
   },
   "outputs": [],
   "source": [
    "plt.plot(results_dqn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN9yy5EWVNz0"
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvRUJUew0yN_"
   },
   "source": [
    "## Part 2\n",
    "\n",
    "Proximal Policy Optimization (https://arxiv.org/pdf/1707.06347.pdf) is a type of policy gradient method. Instead of calculating Q-values, we train a network $\\pi$ to optimize the probability of taking good actions directly, using states as inputs and actions as outputs. PPO also uses a value network $V$ that estimates state values in order to estimate the advantage $\\hat{A}$.\n",
    "\n",
    "Tuples of state, action distribution, action taken, and return $(s,\\pi(s), a,\\hat{R})$ are gathered for several rollouts. After training on this experience, these tuples are discarded and new experience is gathered.\n",
    "\n",
    "Loss for the value network and the policy network are calculated according to the following formula:\n",
    "\n",
    "$Loss=ValueLoss+PolicyLoss$\n",
    "\n",
    "$ValueLoss=\\frac{1}{N}\\sum \\bigg(\\hat{R} - V(s) \\bigg)^2 $\n",
    "\n",
    "$PolicyLoss=-\\frac{1}{N}\\sum \\min\\bigg( \\frac{\\pi'(a|s)}{\\pi(a|s)} \\hat{A}, clip(\\frac{\\pi'(a|s)}{\\pi(a|s)},1-\\epsilon,1+\\epsilon) \\hat{A} \\bigg) $\n",
    "\n",
    "$\\hat{R}_t = \\sum_{i=t}^H \\gamma^{i-t}r_i$\n",
    "\n",
    "$\\hat{A}_t=\\hat{R}_t-V(s_t)$\n",
    "\n",
    "Here, $\\pi'(a|s)$ is the probability of taking an action given a state under the current policy and $\\pi(a|s)$ is the probability of taking an action given a state under the policy used to gather data. In the loss function, $a$ is the action your agent actually took and is sampled from memory.\n",
    "\n",
    "Additionally, the $clip$ function clips the value of the first argument according to the lower and upper bounds in the second and third arguments resectively.\n",
    "\n",
    "Another important note: Your the calculation of your advantage $\\hat{A}$ should not permit gradient flow from your policy loss calculation. In other words, make sure to call `.detach()` on your advantage.\n",
    "\n",
    "### TODO\n",
    "\n",
    "- Implement calculate return method\n",
    "- Implement get action method\n",
    "- Implement PPO learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsm1pILHVcEp"
   },
   "outputs": [],
   "source": [
    "def calculate_return(memory, rollout, gamma):\n",
    "  \"\"\"Return memory with calculated return in experience tuple\n",
    "\n",
    "    Args:\n",
    "        memory (list): (state, action, action_dist, return) tuples\n",
    "        rollout (list): (state, action, action_dist, reward) tuples from last rollout\n",
    "        gamma (float): discount factor\n",
    "\n",
    "    Returns:\n",
    "        list: memory updated with (state, action, action_dist, return) tuples from rollout\n",
    "  \"\"\"\n",
    "  pass\n",
    "\n",
    "\n",
    "def get_action_ppo(network, state):\n",
    "  \"\"\"Sample action from the distribution obtained from the policy network\n",
    "\n",
    "    Args:\n",
    "        network (PolicyNetwork): Policy Network\n",
    "        state (np-array): current state, size (state_size)\n",
    "\n",
    "    Returns:\n",
    "        int: action sampled from output distribution of policy network\n",
    "        array: output distribution of policy network\n",
    "  \"\"\"\n",
    "  pass\n",
    "\n",
    "\n",
    "def learn_ppo(optim, policy, value, memory_dataloader, epsilon, policy_epochs):\n",
    "  \"\"\"Implement PPO policy and value network updates. Iterate over your entire\n",
    "     memory the number of times indicated by policy_epochs.\n",
    "\n",
    "    Args:\n",
    "        optim (Adam): value and policy optimizer\n",
    "        policy (PolicyNetwork): Policy Network\n",
    "        value (ValueNetwork): Value Network\n",
    "        memory_dataloader (DataLoader): dataloader with (state, action, action_dist, return) tensors\n",
    "        epsilon (float): trust region\n",
    "        policy_epochs (int): number of times to iterate over all memory\n",
    "  \"\"\"\n",
    "  pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6RXma_-vSGX"
   },
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8URnP8xvTTG"
   },
   "outputs": [],
   "source": [
    "# Dataset that wraps memory for a dataloader\n",
    "class RLDataset(Dataset):\n",
    "  def __init__(self, data):\n",
    "    super().__init__()\n",
    "    self.data = []\n",
    "    for d in data:\n",
    "      self.data.append(d)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "\n",
    "# Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "  def __init__(self, state_size, action_size):\n",
    "    super().__init__()\n",
    "    hidden_size = 8\n",
    "\n",
    "    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_size, hidden_size),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_size, hidden_size),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_size, action_size),\n",
    "                             nn.Softmax(dim=1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Get policy from state\n",
    "\n",
    "      Args:\n",
    "          state (tensor): current state, size (batch x state_size)\n",
    "\n",
    "      Returns:\n",
    "          action_dist (tensor): probability distribution over actions (batch x action_size)\n",
    "    \"\"\"\n",
    "    return self.net(x)\n",
    "\n",
    "\n",
    "# Value Network\n",
    "class ValueNetwork(nn.Module):\n",
    "  def __init__(self, state_size):\n",
    "    super().__init__()\n",
    "    hidden_size = 8\n",
    "\n",
    "    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_size, hidden_size),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_size, hidden_size),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(hidden_size, 1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Estimate value given state\n",
    "\n",
    "      Args:\n",
    "          state (tensor): current state, size (batch x state_size)\n",
    "\n",
    "      Returns:\n",
    "          value (tensor): estimated value, size (batch)\n",
    "    \"\"\"\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aBD_R_e01Qb"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qX_Bv4M4MyY2"
   },
   "outputs": [],
   "source": [
    "def ppo_main():\n",
    "  # Hyper parameters\n",
    "  lr = 1e-3\n",
    "  epochs = 20\n",
    "  env_samples = 100\n",
    "  gamma = 0.9\n",
    "  batch_size = 256\n",
    "  epsilon = 0.2\n",
    "  policy_epochs = 5\n",
    "\n",
    "  # Init environment\n",
    "  state_size = 4\n",
    "  action_size = 2\n",
    "  env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "  # Init networks\n",
    "  policy_network = PolicyNetwork(state_size, action_size).cuda()\n",
    "  value_network = ValueNetwork(state_size).cuda()\n",
    "\n",
    "  # Init optimizer\n",
    "  optim = torch.optim.Adam(chain(policy_network.parameters(), value_network.parameters()), lr=lr)\n",
    "\n",
    "  # Start main loop\n",
    "  results_ppo = []\n",
    "  loop = tqdm(total=epochs, position=0, leave=False)\n",
    "  for epoch in range(epochs):\n",
    "    last_epoch = (epoch+1 == epochs)\n",
    "    # Record only last epoch\n",
    "    if last_epoch:\n",
    "      env = wrap_env(env)\n",
    "\n",
    "    memory = []  # Reset memory every epoch\n",
    "    rewards = []  # Calculate average episodic reward per epoch\n",
    "\n",
    "    # Begin experience loop\n",
    "    for episode in range(env_samples):\n",
    "\n",
    "      # Reset environment\n",
    "      state, _ = env.reset()\n",
    "      done = False\n",
    "      rollout = []\n",
    "      cum_reward = 0  # Track cumulative reward\n",
    "\n",
    "      # Begin episode\n",
    "      while not done and cum_reward < 200:  # End after 200 steps\n",
    "        # Get action\n",
    "        action, action_dist = get_action_ppo(policy_network, state)\n",
    "\n",
    "        # Take step\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        # env.render()\n",
    "\n",
    "        # Store step\n",
    "        rollout.append((state, action, action_dist, reward))\n",
    "\n",
    "        cum_reward += reward\n",
    "        state = next_state  # Set current state\n",
    "\n",
    "      # Calculate returns and add episode to memory\n",
    "      memory = calculate_return(memory, rollout, gamma)\n",
    "\n",
    "      rewards.append(cum_reward)\n",
    "      env.close()\n",
    "    # Train\n",
    "    dataset = RLDataset(memory)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    learn_ppo(optim, policy_network, value_network, loader, epsilon, policy_epochs)\n",
    "\n",
    "    # Print results\n",
    "    results_ppo.extend(rewards)  # Store rewards for this epoch\n",
    "    loop.update(1)\n",
    "    loop.set_description(\"Epochs: {} Reward: {}\".format(epoch, results_ppo[-1]))\n",
    "\n",
    "  return results_ppo\n",
    "\n",
    "results_ppo = ppo_main()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLXetCMpC1DE"
   },
   "outputs": [],
   "source": [
    "plt.plot(results_ppo)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
