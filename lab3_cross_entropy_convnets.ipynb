{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yjo4uhUQ9Fv"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/porterjenkins/byu-cs474/blob/master/lab3_cross_entropy_convnets.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "# Deep Learning Part 3: Datasets, Data Loading, Cross Entropy, and Convolutional Networks\n",
        "\n",
        "## Grading Standards:\n",
        "*  10%: Dataset/MNIST section\n",
        "*  10%: Correct implementation and use of cross entropy loss\n",
        "*  20%: Correct training/validation functions\n",
        "*  40%: Successful training and validation with MLP and convolution networks\n",
        "*  12%: Convolutional layer quiz\n",
        "*   8%: Comparison between MLP and convolution networks\n",
        "___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wQOefmcZVgTl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUHlLRJ0Q9Fz"
      },
      "source": [
        "Set your global variable `device`, using the `torch.device()` function. In order to use cuda remember to request a GPU from Runtime > Change Runtime.\n",
        "\n",
        "***Important Note**: If you spend too much time or memory on the GPU in Google Colab then you will be timed out. This may not be a big deal with this lab, but it can become a big deal in later labs. It is recommended to set your `device` and Runtime to CPU first and once everything in the lab is working properly to set it to the GPU.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kFAeNl0mQ9F0"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8plxzBbQ9F1"
      },
      "source": [
        "---\n",
        "\n",
        "# Datasets and Data Loading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6A1ZwnczQ9F1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP7rpMjiQ9F2"
      },
      "source": [
        "We are going to make a PyTorch `Dataset`.\n",
        "There are three parts to creating a `Dataset`:\n",
        "1. `__init__()`: This is where you get all relevant data for your dataset.\n",
        "2. `__len__()`: You return how large your dataset is.\n",
        "3. `__getitem__()`: You return an item from your dataset given an index.\n",
        "\n",
        "Implement the TODOs below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x3SGWrkbQ9F3"
      },
      "outputs": [],
      "source": [
        "class SineDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # TODO: The code is same from lab 2, so you can uncomment the code below.\n",
        "        self.x = torch.rand((100,1))*8 - 4\n",
        "        self.y = torch.sin(self.x) + torch.randn_like(self.x)*.1 # the second part of the sum adds noise to the function\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: Return the len of your dataset.\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # TODO: i will be an index so return x_i and y_i\n",
        "        return self.x[i], self.y[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQtRsLxWQ9F3"
      },
      "source": [
        "Now create a `SineDataset` and print out the length of dataset, i.e. `len(dataset)`, and the item in your dataset at index 0, i.e. `dataset[0]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M1HD_XafQ9F4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb8d4fd-76b4-4c7b-a9b8-61765e32a277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "(tensor([-3.3609]), tensor([0.0911]))\n"
          ]
        }
      ],
      "source": [
        "dataset = SineDataset()\n",
        "print(len(dataset))\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ow7q2UEQ9F4"
      },
      "source": [
        "A `DataLoader` uses the `__len__` and `__getitem__` of a `Dataset` to sample indices in `[0, ..., len(dataset)-1]` and collect a batch of items from the `Dataset`.\n",
        "The `DataLoader` will then try to convert the sampled entries into tensors (if they are not already) and concatenate them together.\n",
        "Create a `DataLoader` object below; pass in your dataset and `batch_size=32` as arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8G9BkZUgQ9F4"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHE3O4wwQ9F5"
      },
      "source": [
        "Iterate through your dataloader with a for loop. Because `SineDataset.__getitem__()` returns two items, the for loop will return a tuple.\n",
        "Either unpack the entries in your tuple in the for loop:\n",
        "```python\n",
        "for x, y in dataloader\n",
        "```\n",
        "or after the loop:\n",
        "```python\n",
        "for batch in dataloader:\n",
        "    x, y = batch\n",
        "```\n",
        "\n",
        "Print out the shapes of `x` and `y` for each batch in the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zxJ-K52QQ9F5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02251073-97f5-449b-81b9-20c452c3a21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1]) torch.Size([32, 1])\n",
            "torch.Size([32, 1]) torch.Size([32, 1])\n",
            "torch.Size([32, 1]) torch.Size([32, 1])\n",
            "torch.Size([4, 1]) torch.Size([4, 1])\n"
          ]
        }
      ],
      "source": [
        "for x, y in dataloader:\n",
        "    print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9XGi82cQ9F5"
      },
      "source": [
        "You will notice that the shapes are `(B, Z_in)`, where `B` is batch size and `Z_in` is our input feature size, which is exactly what we want.\n",
        "Also note that the last batch has a batch size of 4; this is because the `DataLoader` samples **without replacement** and these are the last items in our `Dataset` that have not been sampled.\n",
        "\n",
        "Let's now create our real dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORW8H-QwQ9F6"
      },
      "source": [
        "---\n",
        "\n",
        "# MNIST\n",
        "\n",
        "We are now going to look at the MNIST dataset, which is a dataset of handwritten numbers.\n",
        "Our objective will be to create a neural network that can predict the number given the image.\n",
        "\n",
        "First import `torchvision` below so we can retrieve the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ybgv1p4PQ9F6"
      },
      "outputs": [],
      "source": [
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QroZnsvvQ9F7"
      },
      "source": [
        "You can use `torchvision.datasets.MNIST()` to download the MNIST dataset (which inherits the `Dataset` class).\n",
        "For arguments, specify `root=\"/tmp/\"` to denote the location, `train=True` or `train=False` to get the training or test dataset, `download=True` to specify you want to download the dataset, and `transform=torchvision.transforms.ToTensor()` to convert the MNIST images from PIL images to PyTorch tensors.\n",
        "Create both a `train_dataset` and `val_dataset`.\n",
        "\n",
        "*Note: It is good practice to use a train, val, and test dataset, especially in the real world, but in this class we will mainly focus on train and val datasets to simplify things.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BeJcijXkQ9F7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3226b58-94c4-444b-c1ba-ab8701cc2bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /tmp/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 52269666.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /tmp/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 7323359.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /tmp/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 5722119.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /tmp/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7216109.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataset = torchvision.datasets.MNIST(root=\"/tmp/\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "val_dataset = torchvision.datasets.MNIST(root=\"/tmp/\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujysG9TCQ9F7"
      },
      "source": [
        "Print out the lengths of `train_dataset` and `val_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "61Px30_LQ9F8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "974a1a82-2f04-47b8-f176-eb5c58b7aacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fViNzhuQ9F8"
      },
      "source": [
        "Grab element 0 from the `train_dataset`. As a heads up, like our `SineDataset`, `MNIST` returns an image `x` and a class/target `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3QUod3d5Q9F8"
      },
      "outputs": [],
      "source": [
        "x_0, y_0 = train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEJ45xaVQ9F8"
      },
      "source": [
        "Use the `type()` function to see what type of object `x` and `y` are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BH7jijNxQ9F8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1304d23-e2d7-487e-eb03-7515a5c3896d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'int'>\n"
          ]
        }
      ],
      "source": [
        "print(type(x_0))\n",
        "print(type(y_0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8xFEKjZQ9F9"
      },
      "source": [
        "Since x is a tensor, print out its `.dtype`, `.shape`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nspQc1uSQ9F9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570639a8-78df-471e-d139-e4ed231e911f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(x_0.dtype)\n",
        "print(x_0.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV-K1HjMQ9F9"
      },
      "source": [
        "The shape of a tensor image is `(C, H, W)`, where `C` is channels, `H` is height, and `W` is width.\n",
        "In our case `x` has 1 channel and it is a 28x28 image.\n",
        "Because there is 1 channel, it is likely that the image is grayscale (which it is).\n",
        "\n",
        "Now visualize the image and display its class.\n",
        "Use the `plt.imshow()` function to visualize `x`; add the argument `cmap=\"gray\"` to denote the image is grayscale.\n",
        "Use the `plt.title()` function to set the title of the `plt` image to the class `y`.\n",
        "\n",
        "*Note: `plt` expects grayscale images to only have to dimension, HxW, so `.squeeze()` the 0th dimension*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Xq6qHfNZQ9F9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "16588d93-4569-43bd-d50a-36ea3fa2dc6b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(x_0.squeeze(0), cmap=\"gray\")\n",
        "plt.title(y_0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ7Qc7N5Q9F9"
      },
      "source": [
        "Print out the min and max values of `x` using `torch.min()` and `torch.max()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0fU7fzKGQ9F-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53eea544-0010-41d6-b91d-d71e55cf72e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.)\n",
            "tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "print(torch.min(x_0))\n",
        "print(torch.max(x_0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLK7CH_YQ9F-"
      },
      "source": [
        "Our tensors are normalized between 0 and 1, which is good so we don't have to do any normalization.\n",
        "Now that we have a better understanding of our image data, let's examine the classes in the dataset.\n",
        "\n",
        "Create a `get_dataset_classes()` function which takes a dataset as input and count how many times each class appears.\n",
        "Return a dictionary where the keys are the classes and the values represent the number of times each class appears in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RLGxbOj3Q9F-"
      },
      "outputs": [],
      "source": [
        "def get_dataset_classes(dataset):\n",
        "    classes = {}\n",
        "    for _, y in dataset:\n",
        "        if y not in classes:\n",
        "            classes[y] = 0\n",
        "        classes[y] += 1\n",
        "    return classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNlLGb2pQ9F-"
      },
      "source": [
        "Execute the code below to visualize the dataset classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3ZoP7o0JQ9F_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f82bad10-4fb1-4ae2-d4ea-31ccd23a415d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_classes = get_dataset_classes(train_dataset)\n",
        "val_classes = get_dataset_classes(val_dataset)\n",
        "\n",
        "def plot_classes(ax, classes, title):\n",
        "    ax.bar(classes.keys(), classes.values())\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(list(range(10)))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "plot_classes(axes[0], train_classes, \"Train Dataset Classes\")\n",
        "plot_classes(axes[1], val_classes, \"Val Dataset Classes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfg-UTydQ9F_"
      },
      "source": [
        "There are 10 classes, which makes sense because there are 10 digits, 0-9.\n",
        "Furthermore, the classes are pretty evenly spread on both datasets, which means our baseline accuracy is roughly 10%.\n",
        "\n",
        "Now create two `DataLoader` objects called, `train_loader` and `val_loader`.\n",
        "The `train_loader` should have your `train_dataset` and the `val_loader` should have your `val_dataset`.\n",
        "Set the `batch_size` of both dataloaders equal to 32 and set `shuffle=True` for the `train_loader` so that the dataset is shuffled every time.\n",
        "To improve the speed at which you `DataLoader`s can load the data, set `num_workers=4` (for multiprocessing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mLp4yWwXQ9F_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e30458d-8038-45c5-f3d0-c4cfd1088fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEGFonGnQ9GH"
      },
      "source": [
        "Now use `x, y = next(iter(train_loader))` to get a single batch of data from the `train_loader` and print out the shapes and dtypes of `x` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "b_XsyFmUQ9GH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444f5fee-9375-4408-c0bf-6d41b51aa74d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28])\n",
            "torch.Size([32])\n",
            "torch.float32\n",
            "torch.int64\n"
          ]
        }
      ],
      "source": [
        "x, y = next(iter(train_loader))\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(x.dtype)\n",
        "print(y.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpSxXIkQQ9GI"
      },
      "source": [
        "The shape of `x` is `(B, C, H, W)`, where `B` is the batch size. **Always remember, in PyTorch, your data should have a batch dimension.**\n",
        "The shape of `y` is `(B,)` and it is a tensor of type `long`, which is what we want because we are doing classification, which means we want our target to be a class label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY4owfQwm-Ni"
      },
      "source": [
        "___\n",
        "\n",
        "### MLP Network\n",
        "\n",
        "We are now going to make our network.\n",
        "Because we are doing image classification, the input to our network is a batch of images, `shape=(B, C, H, W)`, and the output of our network is a batch of probabilities, `shape=(B, K)`, where `K` represents the number classes in our dataset.\n",
        "In our case `K=10`.\n",
        "\n",
        "We will first try to solve this problem using a fully connected deep network (like your DeepNet from lab 2), sometimes called a Multi-Layer Perceptron (MLP).  \n",
        "Implement an `MLP` below (don't forget to use `nn.Sequential`, `nn.Linear`, `nn.ReLU`).\n",
        "Because `nn.Linear` expects tensors of shape `(B, Z)`, where `Z` is the input feature size, we need to flatten our images. Use the `.view()` function to reshape `x.shape=(B, C, H, W)` into `x.shape=(B, Z)`, where `Z=C*H*W`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ljx0drnxQ9GI"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, out_features, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, out_features)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x.view(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJIluYrhQ9GI"
      },
      "source": [
        "Because we are doing classification, we need to use a different loss function that MSE; cross entropy loss is a good choice and is a common loss function for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trSdeZy2Q9GJ"
      },
      "source": [
        "---\n",
        "\n",
        "# Softmax and CrossEntropy\n",
        "\n",
        "The cross entropy (CE) function is $CE(p, q) = - \\sum p(x) \\log q(x)$, where $p$ and $q$ are probability functions ($p$ is the target probabilities and $q$ is the predicted probabilities) and in our case $x$ represents a class.\n",
        "\n",
        "$p$ represents the target distribution, the true class distribution, which means it is a one-hot vector $p_c$, where $c$ represents the index of the class:\n",
        "$$p_i = \\begin{cases}\n",
        "1, & \\textrm{if } i = c \\\\\n",
        "0, & \\textrm{if } i \\not = c \\\\\n",
        "\\end{cases}$$\n",
        "\n",
        "Then $CE(p, q) = - \\sum_i p_i \\log q_i$ will become $CE(p, q) = - p_c \\log q_c$, because $p_{i \\not = c} = 0$, which is further reduced to $CE(p, q) = - \\log q_c$, since $p_c = 1$. In other words, cross entropy loss for classification is the negative log of the predicted probability of the correct class.\n",
        "\n",
        "Therefore, $p$ is never passed into `F.cross_entropy_loss()`, instead you pass in $q$ you predicted distribution and $c$ the index of the correct class.\n",
        "\n",
        "Implement `prenormalized_cross_entropy_loss` below. $q$ is assumed to be a normalized probability distribution.\n",
        "\n",
        "*Note: Do **not** use a for loop. You can index into a tensor with array slicing (hint: You will need to use `torch.arange()` for the 0th dimension of `q`)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xuhEjtx7Q9GJ"
      },
      "outputs": [],
      "source": [
        "def prenormalized_cross_entropy_loss(q, c):\n",
        "    return -np.log(q[torch.arange(len(q)), c]).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqzoiQDgQ9GK"
      },
      "source": [
        "Validate your function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5l96Vh8HQ9GK"
      },
      "outputs": [],
      "source": [
        "def test_prenormalized_cross_entropy_loss():\n",
        "    q = torch.tensor([[.1, .5, .4],\n",
        "                      [.2, .2, .6],\n",
        "                      [.3, .3, .3]])\n",
        "    c = torch.tensor([2, 0, 1])\n",
        "    assert torch.allclose(prenormalized_cross_entropy_loss(q, c), torch.tensor(1.2432), atol=1e-4)\n",
        "\n",
        "test_prenormalized_cross_entropy_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4pxw0AhQ9GL"
      },
      "source": [
        "This assumed that `q` was a probability distribution, but usually neural networks output logits $l$, which are unnormalized probabilities.\n",
        "One way we could normalize our logits $l$ into probabilities $q$ is to divide $l$ by the sum of logits $q_i = \\frac{l_i}{\\sum_j l_j}$, but that doesn't work if $l_i$ is negative.\n",
        "The softmax, which exponentiates the $logit$, $q_i = \\frac{e^{l_i}}{\\sum_j e^{l_j}}$, before dividing by the sum of exponentiated logits removes the issues of negativity (there are other good reasons for using softmax, such as numerical stability).\n",
        "\n",
        "However, applying `q = softmax(l)` to `prenormalized_cross_entropy_loss(q, c)` can still be numerically unstable.\n",
        "Luckily, we can simplify our function:\n",
        "$$\\begin{align}\n",
        "CE(l, c) &= - \\log \\frac{e^{l_c}}{\\sum_j e^{l_j}} \\\\\n",
        "&= - (\\log e^{l_c} - \\log \\sum_j e^{l_j}) \\\\\n",
        "&= - (l_c - \\log \\sum_j e^{l_j}) \\\\\n",
        "&= - l_c + \\log \\sum_j e^{l_j}\n",
        "\\end{align}$$\n",
        "\n",
        "While you could implement $\\log \\sum_j e^{l_j}$, you should use `torch.logsumexp()` which will exponentiate, sum, and then log your logits, but in a more numerically stable way.\n",
        "Implement `cross_entropy_loss()` below.\n",
        "You can validate it works by comparing it with the output of `F.cross_entropy()`.\n",
        "\n",
        "*Note: Do **not** use a for loop*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eqBTmzaWQ9GL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9e30fb-2fa2-4b55-a1c6-2d3a3742f47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.5180)\n",
            "tensor(1.5180)\n"
          ]
        }
      ],
      "source": [
        "def cross_entropy_loss(l, c):\n",
        "    return (-l[torch.arange(len(l)), c] + torch.logsumexp(l, dim=1)).mean()\n",
        "\n",
        "def test_cross_entropy_loss():\n",
        "    q = torch.tensor([[.1, .5, .4, .0],\n",
        "                      [.2, .2, .6, .9],\n",
        "                      [.3, .3, .3, .3]])\n",
        "    c = torch.tensor([2, 0, 1])\n",
        "\n",
        "    print(cross_entropy_loss(torch.log(q), c))\n",
        "    print(F.cross_entropy(torch.log(q), c))\n",
        "\n",
        "test_cross_entropy_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBEOn1gKQ9GL"
      },
      "source": [
        "It is hard to tell how well a model is performing just from its cross entropy loss, so create a `get_accuracy()` function to measure accuracy.\n",
        "`get_accuracy()` takes in a `y_hat` and `y`, where `y_hat` contains the predicted logits (unnormalized probabilities) for some images `x` and `y` are the labels.\n",
        "You can get the predicted label from `y_hat`, by using the `torch.argmax()` function.\n",
        "\n",
        "*Note: Do **not** use a for loop.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aDoJ9UewQ9GL"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(y_hat, y):\n",
        "    y_prediction = torch.argmax(y_hat, dim=1)\n",
        "    return (y_prediction == y).float().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gliu8YgjQ9GL"
      },
      "source": [
        "## Validation and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVssyecZvWqR"
      },
      "source": [
        "\n",
        "To see how well training is going, implement a `validation()` function to compute the average loss and accuracy over all instances in the `val_loader`.\n",
        "This function will look very similar to a basic training loop, but without any optimization, e.g. no `loss.backward()` or `optimizer.step()`.\n",
        "To speed up the process use `torch.no_grad()` to keep PyTorch from building the computation graph.\n",
        "You can use `torch.no_grad()` either as a decorator:\n",
        "```python\n",
        "@torch.no_grad()\n",
        "def fn():\n",
        "    ...\n",
        "```\n",
        "or as a context manager:\n",
        "```python\n",
        "def fn():\n",
        "    with torch.no_grad():\n",
        "        ...\n",
        "```\n",
        "\n",
        "*Remember you can use `.item()` on a tensor with one element to convert it into a float/int.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "eVpKK9_bQ9GM"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validation(net, val_loader):\n",
        "    # TODO: Return the network's average loss and accuracy on the val loader\n",
        "    net = net.to(device)\n",
        "\n",
        "    x, y = next(iter(val_loader))\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    y_hat = net(x)\n",
        "    loss = cross_entropy_loss(y_hat, y)\n",
        "    accuracy = get_accuracy(y_hat, y)\n",
        "\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RHZYEKNQ9GM"
      },
      "source": [
        "Now implement the `train()` function.\n",
        "This will look similar to the `train()` you implemented in lab 2, but now you will also store training accuracies, and at every `log_val_interval` you will call `validation` and store the validation loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7PQFG1zAQ9GM"
      },
      "outputs": [],
      "source": [
        "def train(net, train_loader, val_loader, optimizer, n_optimization_steps, log_val_interval):\n",
        "    # TODO: Implement training loop and return the training and validation losses and accuracies.\n",
        "    net = net.to(device)\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for i in range(n_optimization_steps):\n",
        "        x, y = next(iter(train_loader))\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        y_hat = net(x)\n",
        "        loss = cross_entropy_loss(y_hat, y)\n",
        "        accuracy = get_accuracy(y_hat, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        accuracies.append(accuracy.item())\n",
        "\n",
        "        if i % log_val_interval == 0:\n",
        "            val_loss, val_accuracy = validation(net, val_loader)\n",
        "            val_losses.append(val_loss.item())\n",
        "            val_accuracies.append(val_accuracy.item())\n",
        "\n",
        "    return losses, accuracies, val_losses, val_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpKn33eFQ9GM"
      },
      "source": [
        "Now train an `MLP` on `MNIST`. To speed up training use the `torch.optim.Adam` optimizer instead of `torch.optim.SGD`.\n",
        "\n",
        "*Note: Don't forget to put your network on `device`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaunNy19Q9GN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbaf9b0-1128-4871-feac-ec20ebcac1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# TODO: Set torch seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# TODO: Initialize your MLP, called net\n",
        "net = MLP(28 * 28, 10, 128)\n",
        "net.to(device)\n",
        "\n",
        "# TODO: Create an Adam optimizer (lr=.001 works well)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=.001)\n",
        "\n",
        "# TODO: Train your MLP for 2000 steps and set log_val_interval=50\n",
        "losses, accuracies, val_losses, val_accuracies = train(\n",
        "    net, train_loader, val_loader, optimizer, 2000, 50\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxlBHEY5Q9GN"
      },
      "source": [
        "Now plot your training and validation loss on the same plot and plot your training and validation accuracies on the same plot.\n",
        "Properly set your x- and y-axis labels and create a legend to make your plot legible.  \n",
        "\n",
        "*Note: that you can specify the x-values for each point by calling `plt.plot(x, y)` instead of `plt.plot(y)`. Since you store validation every 50 steps, you'll need to use `torch.arange` to get the proper x-values to align the validation results to the training results.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "janqsjw6Q9GN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ronkEckHiDaU"
      },
      "source": [
        "___\n",
        "\n",
        "# Convolution Networks\n",
        "\n",
        "Now we will create a convolution network.\n",
        "When we were dealing with an `MLP` we used fully-connected `nn.Linear` layers.\n",
        "In a convolution network we use `nn.Conv2d` layers.\n",
        "`nn.Linear` maps tensors of shape `(B, F_in) -> (B, F_out)`.\n",
        "`nn.Conv2d` maps tensors of shape `(B, C_in, H_in, W_in) -> (B, C_out, H_out, W_out)`, where `C_in` represents our input channels and `C_out` represents our output channels.\n",
        "You decide what `C_out` should be when you initialize.\n",
        "The mapping that\n",
        "\n",
        "If we had a batch of `img` tensors with shape `(B, 3, 8, 8)` and we wanted it to become `(B, 6, 4, 4)` we could create a convolution layer:\n",
        "```python\n",
        "conv_layer = nn.Conv2d(in_channels=3,  # Our 'C_in' which is 3\n",
        "                       out_channels=6, # Our desired 'C_out'\n",
        "                       kernel_size=2,  # One way to make an 8x8 image become a 2x2 image is to have the kernel be 2x2,\n",
        "                       padding=0,      #    with zero padding,\n",
        "                       stride=2,       #    and a stride of 2.\n",
        "                      )\n",
        "```\n",
        "Validate this is true below by creating `conv_prac()` function, which creates a random tensor with shape `(B, 3, 8, 8)`, passes it through `conv_layer`, and prints out the resulting shape.\n",
        "\n",
        "*Note: `kernel_size`, `padding`, and `stride` can all be tuples in case you want different (height, width) parameters, e.g. `kernel=(kernel_height, kernel_width)`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WFcZkkWQ9GN"
      },
      "outputs": [],
      "source": [
        "def conv_prac():\n",
        "    pass\n",
        "\n",
        "conv_prac()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXGRxUQh9gX"
      },
      "source": [
        "___\n",
        "\n",
        "### Quiz\n",
        "Test your knowledge of how convolution layers affect the shape of outputs by answering the following quiz questions.\n",
        "\n",
        "\n",
        "*Using a Kernel size of 3×3 what should the settings of your 2d convolution be that results in the following mappings (first answer given to you)*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : **(out_channels=10, kernel_size=(3, 3), padding=(0, 0))**\n",
        "* (c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=65, h=12, w=12) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=7, h=20, w=20) : **Your answer in bold here**\n",
        "\n",
        "*Using a Kernel size of 5×5:*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : (out_channels=10, kernel_size=(5, 5), padding=(1, 1))\n",
        "* (c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : **Your answer in bold here**\n",
        "\n",
        "*Using Kernel size of 5×3:*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : **Your answer in bold here**\n",
        "\n",
        "*Determine the kernel that requires the smallest padding size to make the following mappings possible:*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=9, w=7) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : **Your answer in bold here**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6iWe-vrQ9GO"
      },
      "source": [
        "## ConvNet\n",
        "\n",
        "Now create a convolution network `ConvNet` that inherits `nn.Module`.\n",
        "The network should have 3 convolution layers (each layer should have 16 output channels):\n",
        "1. Conv Layer 1 should have a 6x6 kernel, no padding, and a stride of 2.\n",
        "2. Conv Layer 2 should have a 4x4 kernel, no padding, and a stride of 2.\n",
        "3. Conv Layer 3 should have a 3x3 kernel, no padding, and a stride of 1.\n",
        "\n",
        "The output of these layers should be a 3x3 image with 16 channels.\n",
        "You should flatten the image (you can use `.view()`, `torch.flatten()`, or `nn.Flatten()`) and then pass it through 2 linear layers:\n",
        "1. Linear Layer 1 should take the flattened image and map it to a vector with 16 features.\n",
        "2. Linear Layer 2 should map its vector to logits.\n",
        "\n",
        "Do not forget to add nonlinearities between the layers (do not add them to the last layer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0_GBKlkQ9GO"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gynh4o0GQ9GP"
      },
      "source": [
        "Train you convolution network below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeFfiCH4Q9GP"
      },
      "outputs": [],
      "source": [
        "# TODO: Set seed for reproducibility\n",
        "\n",
        "# TODO: Initalize your ConvNet, called 'conv_net'\n",
        "conv_net = None\n",
        "\n",
        "# TODO: Create an Adam optimizer (lr=.001 works well)\n",
        "\n",
        "# TODO: Train your ConvNet for 2000 steps and set log_val_interval=50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpvG7fl1Q9GP"
      },
      "source": [
        "Now plot your training and validation loss on the same plot and plot your training and validation accuracies on the same plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC3GifTWQ9GP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlgZfmwRQ9GP"
      },
      "source": [
        "Create a function that outputs the number of parameters in a network.\n",
        "Remember you can call `.parameters()` to recursively retrieve the `Parameter`s in a `Module`.\n",
        "You could then use `.shape` to figure out the number of parameters in a `Parameter` or you could flatten the parameter and get its length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INzCWjI5Q9GP"
      },
      "outputs": [],
      "source": [
        "def get_n_net_params(net):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQR4_hPjQ9GQ"
      },
      "source": [
        "Print the number of parameters and the accuracy of your trained `MLP` and `ConvNet` networks and then write down below:\n",
        "- Which one is more accurate?\n",
        "- Which one is smaller?\n",
        "\n",
        "*Note: You can use `validation()` to get the accuracy of your network.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVCnQ1ccQ9GQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}